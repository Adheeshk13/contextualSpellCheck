{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "import editdistance\n",
    "import datetime\n",
    "\n",
    "from spacy.tokens import Doc, Token, Span\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: ['custom_component', 'tagger', 'parser', 'ner']\n",
      "Doc length: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3411606890003347522"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a custom component\n",
    "def custom_component(doc):\n",
    "    # Print the doc's length\n",
    "    print(\"Doc length:\", len(doc))\n",
    "    # Return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "nlp.add_pipe(custom_component, first=True)\n",
    "\n",
    "# Print the pipeline component names\n",
    "print(\"Pipeline:\", nlp.pipe_names)\n",
    "\n",
    "doc = nlp(\"this is a small test\")\n",
    "doc[1].orth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any([True, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class spellChecker(object):\n",
    "    \"\"\"Class object for Out Of Vocabulary(OOV) corrections \n",
    "    \"\"\"\n",
    "    name = \"contextual spellchecker\"\n",
    "\n",
    "    def __init__(self, debug=False):\n",
    "        self.nlp = spacy.load(\n",
    "            \"en_core_web_sm\", disable=[\"tagger\", \"parser\"]\n",
    "        )  # using default tokeniser with NER\n",
    "        with open(\"./data/vocab.txt\") as f:\n",
    "            # if want to remove '[unusedXX]' from vocab\n",
    "            # words = [line.rstrip() for line in f if not line.startswith('[unused')]\n",
    "            words = [line.rstrip() for line in f]\n",
    "        self.vocab = Vocab(strings=words)\n",
    "        self.BertTokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "        self.BertModel = AutoModelWithLMHead.from_pretrained(\"bert-base-cased\")\n",
    "        self.mask = self.BertTokenizer.mask_token\n",
    "        self.debug = debug\n",
    "        if not Doc.has_extension('contextual_spellCheck'):\n",
    "            Doc.set_extension('contextual_spellCheck', default=True)\n",
    "            Doc.set_extension('performed_spellCheck', default=False)\n",
    "\n",
    "            # {originalToken-1:[suggestedToken-1,suggestedToken-2,..],\n",
    "            #  originalToken-2:[...]}\n",
    "            Doc.set_extension('suggestions_spellCheck', getter=self.doc_suggestions_spellCheck)\n",
    "            Doc.set_extension('outcome_spellCheck', default=\"\")\n",
    "            Doc.set_extension('score_spellCheck', default=None)\n",
    "\n",
    "            Span.set_extension('get_has_spellCheck', getter=self.span_require_spellCheck)\n",
    "            Span.set_extension('score_spellCheck', getter=self.span_score_spellCheck)\n",
    "\n",
    "            Token.set_extension('get_require_spellCheck', getter=self.token_require_spellCheck)\n",
    "            Token.set_extension('get_suggestion_spellCheck', getter=self.token_suggestion_spellCheck)\n",
    "            Token.set_extension('score_spellCheck', getter=self.token_score_spellCheck)\n",
    "    \n",
    "    def token_require_spellCheck(self,token):\n",
    "        \"\"\"Getter for Token attributes. \n",
    "        @Returns True if the token requires spellCheck\n",
    "        \"\"\"\n",
    "        return any([token.orth == suggestion.orth \n",
    "                    for suggestion in token.doc._.suggestions_spellCheck.keys()])\n",
    "    \n",
    "    def token_suggestion_spellCheck(self,token):\n",
    "        \"\"\"Getter for Token attributes. \n",
    "        @Returns    [] or List['suggestion-1','suggestion-1',...] \n",
    "                    \n",
    "        \"\"\"\n",
    "        for suggestion in token.doc._.suggestions_spellCheck.keys():\n",
    "            if token.orth == suggestion.orth:\n",
    "                return token.doc._.suggestions_spellCheck[token]\n",
    "        return []\n",
    "    \n",
    "    def token_score_spellCheck(self, token):\n",
    "        \"\"\"Getter for Token attributes. \n",
    "        @Returns    [] or List[('suggestion-1',score-1), ('suggestion-1',score-2), ...] \n",
    "                    \n",
    "        \"\"\"\n",
    "        for suggestion in token.doc._.score_spellCheck.keys():\n",
    "            if token.orth == suggestion.orth:\n",
    "                return [token.doc._.score_spellCheck[token]]\n",
    "        return []\n",
    "\n",
    "        \n",
    "    def span_score_spellCheck(self, span):\n",
    "        return [{token:self.token_score_spellCheck(token)} for token in span]\n",
    "        \n",
    "    \n",
    "    def span_require_spellCheck(self,span):\n",
    "        \"\"\"Getter for Token attributes. \n",
    "        @Returns True if the span requires spellCheck\n",
    "        \"\"\"\n",
    "        return any([self.token_require_spellCheck(token) for token in span])\n",
    "    \n",
    "    def doc_suggestions_spellCheck(self,doc):\n",
    "        response={}\n",
    "        for token in doc._.score_spellCheck:\n",
    "            if token not in response:\n",
    "                response[token]=[]\n",
    "            for suggestion_score in doc._.score_spellCheck[token]:\n",
    "                response[token].append(suggestion_score[0])       \n",
    "        return response\n",
    "\n",
    "    def check(self, query=\"\"):\n",
    "        \"\"\"Complete pipeline which returns update query\n",
    "\n",
    "        Keyword Arguments:\n",
    "            query {str} -- User query for which spell checking to be done (default: {''})\n",
    "\n",
    "        Returns:\n",
    "            {str} -- returns updated query with spelling corrections (if any)\n",
    "        \"\"\"\n",
    "        if type(query) != str and len(query) == 0:\n",
    "            print(\"Invalid query, expected non empty `str` but passed\", query)\n",
    "\n",
    "        modelLodaded = datetime.datetime.now()\n",
    "        misspellTokens, doc = self.misspellIdentify(query)\n",
    "        modelLoadTime = timeLog(\"Misspell identification: \",modelLodaded)\n",
    "        if len(misspellTokens) > 0:\n",
    "            candidate = self.candidateGenerator(doc, misspellTokens, query=query)\n",
    "            answer = self.candidateRanking(candidate)\n",
    "            updatedQuery = \"\"\n",
    "            for i in doc:\n",
    "                if i in misspellTokens:\n",
    "                    updatedQuery += answer[i] + i.whitespace_\n",
    "                else:\n",
    "                    updatedQuery += i.text_with_ws\n",
    "\n",
    "            print(\"Did you mean: \", updatedQuery)\n",
    "            doc._.set(\"outcome_spellCheck\",updatedQuery)\n",
    "            print(\"Original text:\", query)\n",
    "        return updatedQuery, doc\n",
    "\n",
    "    def misspellIdentify(self, query=\"\"):\n",
    "        \"\"\"To identify misspelled words from the query\n",
    "\n",
    "        At present, All the following criteria should be met for word to be misspelled\n",
    "        1. Should not in our vocab\n",
    "        2. should not be a Person\n",
    "        3. Should not be a number\n",
    "\n",
    "\n",
    "        Keyword Arguments:\n",
    "            query {str} -- user query eg: \"aa bb cc...\" (default: {''})\n",
    "\n",
    "        Returns:\n",
    "            {tuple} -- returns `List[`Token`]` and `Doc`\n",
    "        \"\"\"\n",
    "\n",
    "        doc = self.nlp(query)\n",
    "        misspell = []\n",
    "        for token in doc:\n",
    "            if (\n",
    "                (token.text.lower() not in self.vocab)\n",
    "                and (token.ent_type_ != \"PERSON\")\n",
    "                and (not token.like_num)\n",
    "            ):\n",
    "\n",
    "                misspell.append(token)\n",
    "\n",
    "        if self.debug:\n",
    "            print(misspell)\n",
    "        return (misspell, doc)\n",
    "\n",
    "    def candidateGenerator(self, doc, misspellings, top_n=5, query=\"\"):\n",
    "        \"\"\"Returns Candidates for misspells\n",
    "\n",
    "        This function is responsible for generating candidate list for misspell\n",
    "        using BERT. The misspell is masked with a token and the model tries to \n",
    "        predict `n` candidates for the mask.\n",
    "\n",
    "        Arguments:\n",
    "            misspellings {List[`Token`]} -- Contains List of `Token` object types \n",
    "            from spacy to preserve meta information of the token \n",
    "\n",
    "        Keyword Arguments:\n",
    "            top_n {int} -- Number of candidates to be generated (default: {5})\n",
    "            query {User query} -- This is used for context pwered candidate generations.  (default: {''})\n",
    "\n",
    "        Returns:\n",
    "            Dict{`Token`:List[{str}]} -- Eg of return type {misspell-1:['candidate-1','candidate-2', ...],\n",
    "                            misspell-2:['candidate-1','candidate-2'. ...]}\n",
    "        \"\"\"\n",
    "\n",
    "        response = {}\n",
    "        score = {}\n",
    "\n",
    "        for token in misspellings:\n",
    "            updatedQuery = \"\"\n",
    "            for i in doc:\n",
    "                if (i.i == token.i):\n",
    "                    updatedQuery += self.mask + i.whitespace_\n",
    "                else:\n",
    "                    updatedQuery += i.text_with_ws\n",
    "            if self.debug:\n",
    "                print(\n",
    "                    \"For\", \"`\" + token.text + \"`\", \"updated query is:\\n\", updatedQuery\n",
    "                )\n",
    "\n",
    "            model_input = self.BertTokenizer.encode(updatedQuery, return_tensors=\"pt\")\n",
    "            mask_token_index = torch.where(\n",
    "                model_input == self.BertTokenizer.mask_token_id\n",
    "            )[1]\n",
    "            token_logits = self.BertModel(model_input)[0]\n",
    "            mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "            if self.debug:\n",
    "                print(\"\\nmask_token_logits:\", mask_token_logits)\n",
    "\n",
    "            token_probability = torch.nn.functional.softmax(mask_token_logits,dim=1)\n",
    "            if self.debug: print(\"\\ntoken_probability: \",token_probability)\n",
    "            top_n_score, top_n_tokens = torch.topk(token_probability, top_n, dim=1)\n",
    "            top_n_tokens = top_n_tokens[0].tolist()\n",
    "            top_n_score = top_n_score[0].tolist()\n",
    "            if self.debug:\n",
    "                print(\"top_n_tokens:\", top_n_tokens)\n",
    "                print(\"token_score: \", top_n_score)\n",
    "\n",
    "            if token not in response:\n",
    "                response[token] = [\n",
    "                    self.BertTokenizer.decode([candidateWord])\n",
    "                    for candidateWord in top_n_tokens\n",
    "                ]\n",
    "                score[token]=[(self.BertTokenizer.decode([top_n_tokens[i]]),round(top_n_score[i],5)) for i in range(top_n)]\n",
    "\n",
    "            # for candidate in top_5_tokens:\n",
    "            # response[token].append(self.BertTokenizer.decode([candidate]))\n",
    "            # print(updatedQuery.replace(self.mask, self.BertTokenizer.decode([candidate])))\n",
    "\n",
    "            if self.debug: print(\"\\nresponse: \",response,\"\\nscore: \",score)\n",
    "\n",
    "        doc._.set(\"performed_spellCheck\",True)\n",
    "        doc._.set(\"score_spellCheck\",score)\n",
    "        \n",
    "\n",
    "        return response\n",
    "\n",
    "    def candidateRanking(self, misspellingsDict):\n",
    "        \"\"\"Ranking the candidates based on edit Distance\n",
    "\n",
    "        At present using a library to calculate edit distance \n",
    "        between actual word and candidate words. Candidate word \n",
    "        for which edit distance is lowest is selected. If least \n",
    "        edit distance is same then word with higher probability \n",
    "        is selected by default\n",
    "\n",
    "        Arguments:\n",
    "            misspellingsDict {Dict{`Token`:List[{str}]}} -- \n",
    "            Orginal token is the key and candidate words are the values \n",
    "\n",
    "        Returns:\n",
    "            Dict{`Token`:{str}} -- Eg of return type {misspell-1:'BEST-CANDIDATE'}\n",
    "        \"\"\"\n",
    "\n",
    "        response = {}\n",
    "        #         doc = self.nlp(query)\n",
    "        for misspell in misspellingsDict:\n",
    "            ## Init least_edit distance\n",
    "            least_edit_dist = 100\n",
    "\n",
    "            if self.debug:\n",
    "                print(\"misspellingsDict[misspell]\", misspellingsDict[misspell])\n",
    "            for candidate in misspellingsDict[misspell]:\n",
    "                edit_dist = editdistance.eval(misspell.text, candidate)\n",
    "                if edit_dist < least_edit_dist:\n",
    "                    least_edit_dist = edit_dist\n",
    "                    response[misspell] = candidate\n",
    "\n",
    "            if self.debug:\n",
    "                print(response)\n",
    "        return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeLog(fnName, relativeTime):\n",
    "    \"\"\"For time log\n",
    "\n",
    "    Arguments:\n",
    "        fnName {str} -- function name to print\n",
    "        relativeTime {datetime} -- previous date time for subtraction\n",
    "\n",
    "    Returns:\n",
    "        datetime -- datetime of current logging\n",
    "    \"\"\"\n",
    "\n",
    "    timeNow = datetime.datetime.now()\n",
    "    print(fnName, \"took: \", timeNow - relativeTime)\n",
    "    return datetime.datetime.now()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loading took:  0:00:08.901431\n",
      "[milion, milion]\n",
      "Misspell identification:  took:  0:00:00.005834\n",
      "For `milion` updated query is:\n",
      " Income was $9.4 [MASK] compared to the prior year of $2.7 milion.\n",
      "\n",
      "mask_token_logits: tensor([[-5.2289, -5.2235, -5.4641,  ..., -4.6217, -4.4137, -4.2567]],\n",
      "       grad_fn=<IndexBackward>)\n",
      "\n",
      "token_probability:  tensor([[4.2233e-09, 4.2460e-09, 3.3382e-09,  ..., 7.7504e-09, 9.5427e-09,\n",
      "         1.1165e-08]], grad_fn=<SoftmaxBackward>)\n",
      "top_n_tokens: [1550, 3775, 117, 27005, 16660]\n",
      "token_score:  [0.5942245125770569, 0.2434934675693512, 0.08809225261211395, 0.018352238461375237, 0.008257790468633175]\n",
      "\n",
      "response:  {milion: ['million', 'billion', ',', 'trillion', 'Million']} \n",
      "score:  {milion: [('million', 0.59422), ('billion', 0.24349), (',', 0.08809), ('trillion', 0.01835), ('Million', 0.00826)]}\n",
      "For `milion` updated query is:\n",
      " Income was $9.4 milion compared to the prior year of $2.7 [MASK].\n",
      "\n",
      "mask_token_logits: tensor([[-4.4341, -4.4109, -4.5952,  ..., -4.0313, -3.4055, -3.3470]],\n",
      "       grad_fn=<IndexBackward>)\n",
      "\n",
      "token_probability:  tensor([[2.4615e-09, 2.5194e-09, 2.0953e-09,  ..., 3.6823e-09, 6.8850e-09,\n",
      "         7.2998e-09]], grad_fn=<SoftmaxBackward>)\n",
      "top_n_tokens: [3775, 1550, 27005, 2107, 16660]\n",
      "token_score:  [0.6593362092971802, 0.26184770464897156, 0.05391395464539528, 0.005098216235637665, 0.004251918755471706]\n",
      "\n",
      "response:  {milion: ['million', 'billion', ',', 'trillion', 'Million'], milion: ['billion', 'million', 'trillion', '##M', 'Million']} \n",
      "score:  {milion: [('million', 0.59422), ('billion', 0.24349), (',', 0.08809), ('trillion', 0.01835), ('Million', 0.00826)], milion: [('billion', 0.65934), ('million', 0.26185), ('trillion', 0.05391), ('##M', 0.0051), ('Million', 0.00425)]}\n",
      "misspellingsDict[misspell] ['million', 'billion', ',', 'trillion', 'Million']\n",
      "{milion: 'million'}\n",
      "misspellingsDict[misspell] ['billion', 'million', 'trillion', '##M', 'Million']\n",
      "{milion: 'million', milion: 'million'}\n",
      "Did you mean:  Income was $9.4 million compared to the prior year of $2.7 million.\n",
      "Original text: Income was $9.4 milion compared to the prior year of $2.7 milion.\n",
      "Sentence Correction took:  0:00:00.260853\n"
     ]
    }
   ],
   "source": [
    "start=datetime.datetime.now()\n",
    "checker = spellChecker(debug=True)\n",
    "modelLoadTime = timeLog(\"Model Loading\",start)\n",
    "\n",
    "query = \"Income was $9.4 milion compared to the prior year of $2.7 milion.\"\n",
    "\n",
    "(updatedQuery, doc) = checker.check(query)\n",
    "checkerTime = timeLog('Sentence Correction', modelLoadTime)\n",
    "\n",
    "\n",
    "# misspellTokens = checker.misspellIdentify()\n",
    "# misspellTime = timeLog(\"Misspell indetifying\", modelLoadTime)\n",
    "\n",
    "# candidate = checker.candidateGenerator(misspellTokens)\n",
    "# candidateTime = timeLog(\"CandidateGeneration\",misspellTime)\n",
    "\n",
    "# answer = checker.candidateRanking(candidate)\n",
    "# timeLog(\"ranking\",candidateTime)\n",
    "# for key in answer:\n",
    "#     print('wrong spelling: ','`'+key.text+'`',\"-- best candidate:\", '`'+answer[key]+'`')\n",
    "# print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Doc Extention Test ====================\n",
      "True\n",
      "True\n",
      "{milion: ['million', 'billion', ',', 'trillion', 'Million'], milion: ['billion', 'million', 'trillion', '##M', 'Million']}\n",
      "Income was $9.4 million compared to the prior year of $2.7 million.\n",
      "{milion: [('million', 0.59422), ('billion', 0.24349), (',', 0.08809), ('trillion', 0.01835), ('Million', 0.00826)], milion: [('billion', 0.65934), ('million', 0.26185), ('trillion', 0.05391), ('##M', 0.0051), ('Million', 0.00425)]}\n",
      "==================== Token Extention Test ====================\n",
      "True milion\n",
      "True\n",
      "['billion', 'million', 'trillion', '##M', 'Million']\n",
      "[[('billion', 0.65934), ('million', 0.26185), ('trillion', 0.05391), ('##M', 0.0051), ('Million', 0.00425)]]\n",
      "==================== Span Extention Test ====================\n",
      "True milion\n",
      "True\n",
      "[{prior: []}, {year: []}, {of: []}, {$: []}, {2.7: []}, {milion: [[('billion', 0.65934), ('million', 0.26185), ('trillion', 0.05391), ('##M', 0.0051), ('Million', 0.00425)]]}]\n"
     ]
    }
   ],
   "source": [
    "#             Doc.set_extension('contextual_spellCheck', default=True)\n",
    "#             Doc.set_extension('performed_spellCheck', default=False)\n",
    "\n",
    "#             # {originalToken-1:[suggestedToken-1,suggestedToken-2,..],\n",
    "#             #  originalToken-2:[...]}\n",
    "#             Doc.set_extension('suggestions_spellCheck', default=None)\n",
    "#             Doc.set_extension('outcome_spellCheck', default=\"\")\n",
    "#             Doc.set_extension('score_spellCheck', default=None)\n",
    "\n",
    "#             Span.set_extension('get_has_spellCheck', getter=self.span_require_spellCheck)\n",
    "#             Span.set_extension('score_spellCheck', getter=self.span_score_spellCheck)\n",
    "\n",
    "#             Token.set_extension('get_require_spellCheck', getter=self.token_require_spellCheck)\n",
    "#             Token.set_extension('get_suggestion_spellCheck', getter=self.token_suggestion_spellCheck)\n",
    "#             Token.set_extension('score_spellCheck', getter=self.token_score_spellCheck)\n",
    "# #             Token.set_extension('score_spellCheck', default=1.0)\n",
    "\n",
    "\n",
    "print(\"=\"*20,\"Doc Extention Test\", \"=\"*20)\n",
    "print(doc._.contextual_spellCheck)\n",
    "print(doc._.performed_spellCheck)\n",
    "print(doc._.suggestions_spellCheck)\n",
    "print(doc._.outcome_spellCheck)\n",
    "print(doc._.score_spellCheck)\n",
    "\n",
    "print(\"=\"*20,\"Token Extention Test\", \"=\"*20)\n",
    "print(checker.token_require_spellCheck(doc[len(doc)-2]),doc[len(doc)-2].text)\n",
    "print(doc[len(doc)-2]._.get_require_spellCheck)\n",
    "print(doc[len(doc)-2]._.get_suggestion_spellCheck)\n",
    "print(doc[len(doc)-2]._.score_spellCheck)\n",
    "\n",
    "\n",
    "print(\"=\"*20,\"Span Extention Test\", \"=\"*20)\n",
    "print(checker.token_require_spellCheck(doc[len(doc)-2]),doc[len(doc)-2].text)\n",
    "print(doc[len(doc)-7:len(doc)-1]._.get_has_spellCheck)\n",
    "print(doc[len(doc)-7:len(doc)-1]._.score_spellCheck)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[2].is_oov)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[0.9035419225692749, 0.09310110658407211, 0.0012437802506610751, 0.0006837246473878622, 0.0005358955240808427]\n",
    "a=[round(i,5) for i in a]\n",
    "print(a)\n",
    "\n",
    "b = [(a[i]+1,a[i]) for i in range(5)]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD CODE\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
