{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "import editdistance\n",
    "import datetime\n",
    "\n",
    "from spacy.tokens import Token\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class oovChecker:\n",
    "    \"\"\"Class object for Out Of Vocabulary(OOV) corrections \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, debug=False):\n",
    "        self.nlp = spacy.load(\n",
    "            \"en_core_web_sm\", disable=[\"tagger\", \"parser\"]\n",
    "        )  # using default tokeniser with NER \n",
    "        with open(\"./data/vocab.txt\") as f:\n",
    "            # if want to remove '[unusedXX]' from vocab\n",
    "            # words = [line.rstrip() for line in f if not line.startswith('[unused')]\n",
    "            words = [line.rstrip() for line in f]\n",
    "        self.bigram={}\n",
    "        with open(\"./data/bigram_norwig.txt\") as f:\n",
    "            # if want to remove '[unusedXX]' from vocab\n",
    "            # words = [line.rstrip() for line in f if not line.startswith('[unused')]\n",
    "            for line in f:\n",
    "                bigram, count = line.split('\\t')\n",
    "                # bigram has format 'word1 word2'\n",
    "                self.bigram[bigram] = int(count)\n",
    "        self.vocab = Vocab(strings=words)\n",
    "        self.BertTokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "        self.BertModel = AutoModelWithLMHead.from_pretrained(\"bert-base-cased\")\n",
    "        self.mask = self.BertTokenizer.mask_token\n",
    "        self.debug = debug\n",
    "\n",
    "    def check(self, query=\"\"):\n",
    "        \"\"\"Complete pipeline which returns update query\n",
    "\n",
    "        Keyword Arguments:\n",
    "            query {str} -- User query for which spell checking to be done (default: {''})\n",
    "\n",
    "        Returns:\n",
    "            {str} -- returns updated query with spelling corrections (if any)\n",
    "        \"\"\"\n",
    "        if type(query) != str and len(query) == 0:\n",
    "            print(\"Invalid query, expected non empty `str` but passed\", query)\n",
    "\n",
    "        misspellTokens, doc = self.misspellIdentify(query)\n",
    "        if len(misspellTokens) > 0:\n",
    "            candidate = self.candidateGenerator(misspellTokens, query=query)\n",
    "            answer = self.candidateRanking(candidate)\n",
    "            updatedQuery = \"\"\n",
    "            for i in doc:\n",
    "                if i in misspellTokens:\n",
    "                    updatedQuery += answer[i] + \" \"\n",
    "                else:\n",
    "                    updatedQuery += i.text + \" \"\n",
    "\n",
    "            print(\"Did you mean: \", updatedQuery)\n",
    "            print(\"Original text:\", query)\n",
    "        return updatedQuery\n",
    "\n",
    "    def misspellIdentify(self, query=\"\"):\n",
    "        \"\"\"To identify misspelled words from the query\n",
    "\n",
    "        At present, All the following criteria should be met for word to be misspelled\n",
    "        1. Should not in our vocab\n",
    "        2. should not be a Person\n",
    "        3. Should not be a number\n",
    "\n",
    "\n",
    "        Keyword Arguments:\n",
    "            query {str} -- user query eg: \"aa bb cc...\" (default: {''})\n",
    "\n",
    "        Returns:\n",
    "            {tuple} -- returns `List[`Token`]` and `Doc`\n",
    "        \"\"\"\n",
    "\n",
    "        doc = self.nlp(query)\n",
    "        misspell = []\n",
    "        for token in doc:\n",
    "            if (\n",
    "                (token.text.lower() not in self.vocab)\n",
    "                and (token.ent_type_ != \"PERSON\")\n",
    "                and (not token.like_num)\n",
    "            ):\n",
    "\n",
    "                misspell.append(token)\n",
    "\n",
    "        if self.debug:\n",
    "            print(misspell)\n",
    "        return (misspell, doc)\n",
    "\n",
    "    def candidateGenerator(self, misspellings, top_n=5, query=\"\"):\n",
    "        \"\"\"Returns Candidates for misspells\n",
    "\n",
    "        This function is responsible for generating candidate list for misspell\n",
    "        using BERT. The misspell is masked with a token and the model tries to \n",
    "        predict `n` candidates for the mask.\n",
    "\n",
    "        Arguments:\n",
    "            misspellings {List[`Token`]} -- Contains List of `Token` object types \n",
    "            from spacy to preserve meta information of the token \n",
    "\n",
    "        Keyword Arguments:\n",
    "            top_n {int} -- Number of candidates to be generated (default: {5})\n",
    "            query {User query} -- This is used for context pwered candidate generations.  (default: {''})\n",
    "\n",
    "        Returns:\n",
    "            Dict{`Token`:List[{str}]} -- Eg of return type {misspell-1:['candidate-1','candidate-2', ...],\n",
    "                            misspell-2:['candidate-1','candidate-2'. ...]}\n",
    "        \"\"\"\n",
    "\n",
    "        response = {}\n",
    "\n",
    "        for token in misspellings:\n",
    "            updatedQuery = query\n",
    "            updatedQuery = updatedQuery.replace(token.text, self.mask)\n",
    "            if self.debug:\n",
    "                print(\n",
    "                    \"For\", \"`\" + token.text + \"`\", \"updated query is:\\n\", updatedQuery\n",
    "                )\n",
    "\n",
    "            model_input = self.BertTokenizer.encode(updatedQuery, return_tensors=\"pt\")\n",
    "            mask_token_index = torch.where(\n",
    "                model_input == self.BertTokenizer.mask_token_id\n",
    "            )[1]\n",
    "            token_logits = self.BertModel(model_input)[0]\n",
    "            mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "            top_n_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "            if self.debug:\n",
    "                print(\"top_n_tokens:\", top_n_tokens)\n",
    "\n",
    "            if token not in response:\n",
    "                response[token] = [\n",
    "                    self.BertTokenizer.decode([candidateWord])\n",
    "                    for candidateWord in top_n_tokens\n",
    "                ]\n",
    "\n",
    "            # for candidate in top_5_tokens:\n",
    "            # response[token].append(self.BertTokenizer.decode([candidate]))\n",
    "            # print(updatedQuery.replace(self.mask, self.BertTokenizer.decode([candidate])))\n",
    "\n",
    "            if self.debug:\n",
    "                print(response)\n",
    "\n",
    "        return response\n",
    "\n",
    "    def candidateRanking(self, misspellingsDict):\n",
    "        \"\"\"Ranking the candidates based on edit Distance\n",
    "\n",
    "        At present using a library to calculate edit distance \n",
    "        between actual word and candidate words. Candidate word \n",
    "        for which edit distance is lowest is selected. If least \n",
    "        edit distance is same then word with higher probability \n",
    "        is selected by default\n",
    "\n",
    "        Arguments:\n",
    "            misspellingsDict {Dict{`Token`:List[{str}]}} -- \n",
    "            Orginal token is the key and candidate words are the values \n",
    "\n",
    "        Returns:\n",
    "            Dict{`Token`:{str}} -- Eg of return type {misspell-1:'BEST-CANDIDATE'}\n",
    "        \"\"\"\n",
    "\n",
    "        response = {}\n",
    "        #         doc = self.nlp(query)\n",
    "        for misspell in misspellingsDict:\n",
    "            ## Init least_edit distance\n",
    "            least_edit_dist = 100\n",
    "\n",
    "            if self.debug:\n",
    "                print(\"misspellingsDict[misspell]\", misspellingsDict[misspell])\n",
    "            for candidate in misspellingsDict[misspell]:\n",
    "                edit_dist = editdistance.eval(misspell.text, candidate)\n",
    "                if edit_dist < least_edit_dist:\n",
    "                    least_edit_dist = edit_dist\n",
    "                    response[misspell] = candidate\n",
    "\n",
    "            if self.debug:\n",
    "                print(response)\n",
    "        return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeLog(fnName, relativeTime):\n",
    "    \"\"\"For time log\n",
    "\n",
    "    Arguments:\n",
    "        fnName {str} -- function name to print\n",
    "        relativeTime {datetime} -- previous date time for subtraction\n",
    "\n",
    "    Returns:\n",
    "        datetime -- datetime of current logging\n",
    "    \"\"\"\n",
    "\n",
    "    timeNow = datetime.datetime.now()\n",
    "    print(fnName, \"took: \", timeNow - relativeTime)\n",
    "    return datetime.datetime.now()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4387f7e33fc44dca1ef60d693f6d04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Loading took:  0:00:11.064116\n",
      "Did you mean:  Income was $ 9.4 million compared to the prior year of $ 2.7 million . \n",
      "Original text: Income was $9.4 million compared to the prior year of $2.7 milion.\n",
      "Sentence Correction took:  0:00:00.447252\n"
     ]
    }
   ],
   "source": [
    "start=datetime.datetime.now()\n",
    "checker = oovChecker()\n",
    "modelLoadTime = timeLog(\"Model Loading\",start)\n",
    "\n",
    "query = \"Income was $9.4 million compared to the prior year of $2.7 milion.\"\n",
    "\n",
    "checker.check(query)\n",
    "checkerTime = timeLog('Sentence Correction', modelLoadTime)\n",
    "\n",
    "\n",
    "# misspellTokens = checker.misspellIdentify()\n",
    "# misspellTime = timeLog(\"Misspell indetifying\", modelLoadTime)\n",
    "\n",
    "# candidate = checker.candidateGenerator(misspellTokens)\n",
    "# candidateTime = timeLog(\"CandidateGeneration\",misspellTime)\n",
    "\n",
    "# answer = checker.candidateRanking(candidate)\n",
    "# timeLog(\"ranking\",candidateTime)\n",
    "# for key in answer:\n",
    "#     print('wrong spelling: ','`'+key.text+'`',\"-- best candidate:\", '`'+answer[key]+'`')\n",
    "# print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams={}\n",
    "with open(\"./data/bigram_norwig.txt\") as f:\n",
    "    # if want to remove '[unusedXX]' from vocab\n",
    "    # words = [line.rstrip() for line in f if not line.startswith('[unused')]\n",
    "    for line in f:\n",
    "        bigram, count = line.split('\\t')\n",
    "        # bigram has format 'word1 word2'\n",
    "        bigrams[bigram] = int(count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Income is $9.4 million compared to the prior year of $2.7 milion.\n",
      "<s> income not in bigram dict\n",
      "is $ not in bigram dict\n",
      "$ 9.4 not in bigram dict\n",
      "9.4 million not in bigram dict\n",
      "million compared 218689\n",
      "compared to 18414908\n",
      "to the 1139248999\n",
      "the prior 5085932\n",
      "prior year 823624\n",
      "year of 10234915\n",
      "of $ not in bigram dict\n",
      "$ 2.7 not in bigram dict\n",
      "2.7 milion not in bigram dict\n",
      "milion . not in bigram dict\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('<S>', Income),\n",
       " (is, $),\n",
       " ($, 9.4),\n",
       " (9.4, million),\n",
       " (million, compared),\n",
       " (compared, to),\n",
       " (to, the),\n",
       " (the, prior),\n",
       " (prior, year),\n",
       " (year, of),\n",
       " (of, $),\n",
       " ($, 2.7),\n",
       " (2.7, milion),\n",
       " (milion, .)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\"])\n",
    "query = \"Income is $9.4 million compared to the prior year of $2.7 milion.\"\n",
    "query2=\"Disappointment turned to dire straits when Vestronâ€™s\"\n",
    "\n",
    "doc=nlp(query)\n",
    "def ngram(doc):\n",
    "    bigram=[]\n",
    "#     print(doc[0])\n",
    "    for i in range(len(doc)-1):\n",
    "        if i==0:\n",
    "            bigram.append((\"<S>\",doc[i]))\n",
    "#             continue\n",
    "        else:\n",
    "            bigram.append((doc[i],doc[i+1]))\n",
    "        \n",
    "        joined = ' '.join(str(i).lower() for i in bigram[i])\n",
    "#         print(joined)\n",
    "        if(joined in bigrams):\n",
    "            print(joined,bigrams[joined])\n",
    "        else:\n",
    "            print(joined, 'not in bigram dict')\n",
    "        \n",
    "    return bigram\n",
    "print(query)\n",
    "ngram(doc)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'<S> turned' in bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Start Is For Life'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=\"start is for life\"\n",
    "a.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD CODE\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
